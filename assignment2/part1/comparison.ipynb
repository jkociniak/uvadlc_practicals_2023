{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "(a) (4 points) Plot for VGG11, VGG11 with batch normalization, ResNet18, ResNet34, DenseNet121 and MobileNet-v3-Small, the Top-1 accuracy on ImageNet vs the inference speed. The value for the Top-1 accuracy of each model can be found on the PyTorch website. Also plot the inference speed vs the number of parameters. Does it scale proportionately? Make sure to set the model on evaluation mode, use `torch.no_grad()` and a GPU. Report the inference speed in ms for one image. Average the inference speed across multiple forward passes. Shortly, describe the trends you observe."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import timm\n",
    "\n",
    "# set the seed for reproducibility of the whole notebook\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def vit_s_8():\n",
    "    \"\"\"ViT-S/8 is not a default torchvision model, so we provide it by timm\"\"\"\n",
    "    # Accuracy approximation comes from\n",
    "    # https://openreview.net/pdf?id=LtKcMgGOeLt\n",
    "    # and DINO\n",
    "    # https://arxiv.org/abs/2104.14294\n",
    "    return timm.create_model('vit_small_patch8_224', pretrained=True)\n",
    "\n",
    "# import models and weights\n",
    "from torchvision.models import (\n",
    "    vit_b_32, ViT_B_32_Weights,\n",
    "    vgg11, VGG11_Weights,\n",
    "    vgg11_bn, VGG11_BN_Weights,\n",
    "    resnet18, ResNet18_Weights,\n",
    "    densenet121, DenseNet121_Weights,\n",
    "    mobilenet_v3_small, MobileNet_V3_Small_Weights\n",
    ")\n",
    "\n",
    "# models data (name, constructor fn, weights)\n",
    "models = [\n",
    "    ('ViT-S/8', vit_s_8, None),\n",
    "    ('ViT-B/32', vit_b_32, ViT_B_32_Weights),\n",
    "    ('VGG11', vgg11, VGG11_Weights),\n",
    "    ('VGG11 (BN)', vgg11_bn, VGG11_BN_Weights),\n",
    "    ('ResNet18', resnet18, ResNet18_Weights),\n",
    "    ('DenseNet121', densenet121, DenseNet121_Weights),\n",
    "    ('MobileNet V3', mobilenet_v3_small, MobileNet_V3_Small_Weights)\n",
    "]\n",
    "\n",
    "model_accs = {\n",
    "    'vit_s_8': 80., # Approximated\n",
    "    'vit_b_32' : 75.912,\n",
    "    'vgg11' : 69.02,\n",
    "    'vgg11_bn' : 70.37,\n",
    "    'resnet18' : 69.758,\n",
    "    'densenet121' : 74.434,\n",
    "    'mobilenet_v3_small' : 67.668,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def run_inference(device, img, model_ctor, weights, grad_enabled=False, n_reps=10):\n",
    "    # load model and preprocessing transforms\n",
    "    if weights is None:  # vit-s/8 is loaded via timm\n",
    "        model = model_ctor()\n",
    "        data_config = timm.data.resolve_model_data_config(model)\n",
    "        transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "    else:\n",
    "        weights = weights.IMAGENET1K_V1\n",
    "        model = model_ctor(weights=weights, progress=False)\n",
    "        transforms = weights.transforms()\n",
    "\n",
    "    # put everything on the device\n",
    "    model.to(device)\n",
    "    img.to(device)\n",
    "\n",
    "    # set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    # init gpu loggers\n",
    "    starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "    timings = torch.zeros(n_reps)\n",
    "\n",
    "    # GPU-WARM-UP\n",
    "    for _ in range(10):\n",
    "        _ = model(img)\n",
    "\n",
    "    # run inference\n",
    "    with torch.set_grad_enabled(grad_enabled):\n",
    "        for i in range(n_reps):\n",
    "            transformed_img = transforms(img)\n",
    "            starter.record()\n",
    "            y_pred_probs = model(transformed_img)\n",
    "            y_pred_probs.argmax(dim=1)\n",
    "            ender.record()\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            elapsed_time = starter.elapsed_time(ender)\n",
    "            timings[i] = elapsed_time\n",
    "\n",
    "    return timings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/86.7M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aeeb0a763fc64f0fb3d6f93e7370a3ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tried to instantiate dummy base class Event",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m n_reps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model_name, model_ctor, weights \u001B[38;5;129;01min\u001B[39;00m models[:\u001B[38;5;241m1\u001B[39m]:\n\u001B[0;32m----> 8\u001B[0m     timings \u001B[38;5;241m=\u001B[39m \u001B[43mrun_inference\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel_ctor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_enabled\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgrad_enabled\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_reps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_reps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m     timings \u001B[38;5;241m=\u001B[39m timings\u001B[38;5;241m.\u001B[39mmean()\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInference time for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (grad disabled, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mn_reps\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m images): \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtimings\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 22\u001B[0m, in \u001B[0;36mrun_inference\u001B[0;34m(device, img, model_ctor, weights, grad_enabled, n_reps)\u001B[0m\n\u001B[1;32m     19\u001B[0m model\u001B[38;5;241m.\u001B[39meval()\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# init gpu loggers\u001B[39;00m\n\u001B[0;32m---> 22\u001B[0m starter, ender \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcuda\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEvent\u001B[49m\u001B[43m(\u001B[49m\u001B[43menable_timing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m, torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mEvent(enable_timing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     23\u001B[0m timings \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros(n_reps)\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# GPU-WARM-UP\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dl2023/lib/python3.11/site-packages/torch/cuda/streams.py:163\u001B[0m, in \u001B[0;36mEvent.__new__\u001B[0;34m(cls, enable_timing, blocking, interprocess)\u001B[0m\n\u001B[1;32m    162\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__new__\u001B[39m(\u001B[38;5;28mcls\u001B[39m, enable_timing\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, blocking\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, interprocess\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__new__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable_timing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_timing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mblocking\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mblocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43minterprocess\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterprocess\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/dl2023/lib/python3.11/site-packages/torch/cuda/_utils.py:49\u001B[0m, in \u001B[0;36m_dummy_type.<locals>.get_err_fn.<locals>.err_fn\u001B[0;34m(obj, *args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     48\u001B[0m     class_name \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\n\u001B[0;32m---> 49\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTried to instantiate dummy base class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mclass_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Tried to instantiate dummy base class Event"
     ]
    }
   ],
   "source": [
    "#assert torch.cuda.is_available(), \"Inference should be run on GPU!\"\n",
    "device = torch.device('mps')\n",
    "img = torch.rand(224, 224)\n",
    "grad_enabled = False\n",
    "n_reps = 1\n",
    "\n",
    "for model_name, model_ctor, weights in models[:1]:\n",
    "    timings = run_inference(device, img, model_ctor, weights, grad_enabled=grad_enabled, n_reps=n_reps)\n",
    "    timings = timings.mean()\n",
    "    print(f'Inference time for {model_name} (grad disabled, {n_reps} images): {timings}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "(b) (2 points) Do you expect the inference speed to increase or decrease without torch.no_grad()? Why? What does torch.no_grad() do? For the same models as in (a), plot the inference speed with and without torch.no_grad()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img = torch.rand(224, 224)\n",
    "grad_enabled = True\n",
    "n_reps = 1\n",
    "\n",
    "timings = {}\n",
    "for model_name, model_ctor, weights in models:\n",
    "    timings[model_name] = run_inference(device, img, model_ctor, weights, grad_enabled, n_reps)\n",
    "\n",
    "for k, v in timings.items():\n",
    "    print(f'Inference time for {k} (grad enabled, {n_reps} images): {v}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "c) (2 points) For the same models as in (a), plot the amount of GPU vRAM (you can check this with code by executing torch.cuda.memory_allocated() or with the terminal using nvidia-smi) while conducting a forward pass with torch.no_grad() and without. Does torch.no_grad() influence the memory usage? Why? Make sure to save the output after the forward pass. Use batch_size=64 and report the memory in MB.\n",
    "\n",
    "Hint: You can create a fake image with torch.rand()."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
